if (is.na(newData$steps[i])) {
newData$steps[i] <- avgSteps[which(newData$interval[i] == avgSteps$interval), ]$meanOfSteps
}
}
head(newData)
sum(is.na(newData))
```
* Make a histogram of the total number of steps taken each day and Calculate and report the mean and median total number of steps taken per day.
```{r}
ggplot(newData, aes(date, steps)) + geom_bar(stat = "identity",
colour = "steelblue",
fill = "steelblue",
width = 0.7) + facet_grid(. ~ month, scales = "free") + labs(title = "Histogram of Total Number of Steps Taken Each Day (no missing data)", x = "Date", y = "Total number of steps")
```
* Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?
Mean total number of steps taken per day:
```{r}
newTotalSteps <- aggregate(newData$steps,
list(Date = newData$date),
FUN = "sum")$x
newMean <- mean(newTotalSteps)
newMean
```
Median total number of steps taken per day:
```{r}
newMedian <- median(newTotalSteps)
newMedian
```
Compare them with the two before imputing missing data:
```{r}
oldMean <- mean(totalSteps)
oldMedian <- median(totalSteps)
newMean - oldMean
newMedian - oldMedian
```
So, after imputing the missing data, the new mean of total steps taken per day is the same as that of the old mean; the new median of total steps taken per day is greater than that of the old median.
### Are there differences in activity patterns between weekdays and weekends?
* Create a new factor variable in the dataset with two levels -- "weekday" and "weekend" indicating whether a given date is a weekday or weekend day.
```{r}
head(newData)
newData$weekdays <- factor(format(newData$date, "%A"))
levels(newData$weekdays)
levels(newData$weekdays) <- list(weekday = c("Monday", "Tuesday",
"Wednesday",
"Thursday", "Friday"),
weekend = c("Saturday", "Sunday"))
levels(newData$weekdays)
table(newData$weekdays)
```
* Make a panel plot containing a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday days or weekend days (y-axis).
```{r}
avgSteps <- aggregate(newData$steps,
list(interval = as.numeric(as.character(newData$interval)),
weekdays = newData$weekdays),
FUN = "mean")
names(avgSteps)[3] <- "meanOfSteps"
library(lattice)
xyplot(avgSteps$meanOfSteps ~ avgSteps$interval | avgSteps$weekdays,
layout = c(1, 2), type = "l",
xlab = "Interval", ylab = "Number of steps")
```
pollutantmean <- function(directory, pollutant, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'pollutant' is a character vector of length 1 indicating
## the name of the pollutant for which we will calculate the
## mean; either "sulfate" or "nitrate".
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return the mean of the pollutant across all monitors list
## in the 'id' vector (ignoring NA values)
allFiles <- list.files(path = "C:/Users/vouri_000/Desktop/coursera/2-R Programming/week2/rprog-data-specdata/specdata", full.names = TRUE)
selectedData <- data.frame()
for (i in id) {
selectedData <- rbind(selectedData, read.csv(allFiles[i]))
}
if (pollutant == 'sulfate') {
mean(selectedData$sulfate, na.rm = TRUE)
} else if (pollutant == 'nitrate') {
mean(selectedData$nitrate, na.rm = TRUE)
}
}
source("pollutantmean.R")
pollutantmean("specdata", "sulfate", 1:10)
x <- matrix(rnorm(200), 20, 10)
View(x)
apply(x, 2, mean)
z<-apply(x, 2, mean)
install.packages("datasets")
library(datasets)
data(iris)
?iris
mean(iris[iris$Species == "virginica",]$Sepal.Length)
apply(iris[, 1:4], 2, mean)
library(datasets)
data(mtcars)
x <- matrix(rnorm(200), 20, 10)
?mtcars
sapply(split(mtcars$mpg, mtcars$cyl), mean)
split(mtcars, mtcars$cyl)
tapply(mtcars$mpg, mtcars$cyl, mean)
mean(mtcars[mtcars$cyl == "8",]$hp) - mean(mtcars[mtcars$cyl == "4",]$hp)
debug(ls)
ls
makeCacheMatrix <- function(mtx = matrix()) {
inverse <- NULL
set <- function(x) {
mtx <<- x;
inverse <<- NULL;
}
get <- function() return(mtx);
setinv <- function(inv) inverse <<- inv;
getinv <- function() return(inverse);
return(list(set = set, get = get, setinv = setinv, getinv = getinv))
}
cacheSolve <- function(mtx, ...) {
inverse <- mtx$getinv()
if(!is.null(inverse)) {
message("Getting cached data...")
return(inverse)
}
data <- mtx$get()
invserse <- solve(data, ...)
mtx$setinv(inverse)
return(inverse)
}
x_bar <- 1100
s <- 30
n <- 9
alpha <- 0.05
ts <- qt(1 - alpha / 2, n - 1) # 2.306004
round(x_bar + c(-1, 1) * ts * s / sqrt(n)) # 1077 1123
x_bar <- -2
n <- 9
alpha <- 0.05
ts <- qt(1 - alpha / 2, n - 1) # 2.306004
s <- -x_bar*sqrt(n) / ts
s # 2.601903
n_x <- 10
n_y <- 10
x_bar <- 5 # old_system
y_bar <- 3 # new_system
var_x <- 0.6
var_y <- 0.68
alpha <- 0.05
sp_2 <- ((n_x - 1)*var_x + (n_y - 1)*var_y) / (n_x + n_y - 2)
sp <- sqrt(sp_2)
ts <- qt(1 - (alpha/2), n_x + n_y - 2)
round((y_bar - x_bar) + c(-1, 1) * ts * sp * (sqrt(1/n_x + 1/n_y)), 2)
n_x <- 100
n_y <- 100
x_bar <- 6
y_bar <- 4
s_x <- 2
s_y <- 0.5
alpha <- 0.05
sp_2 <- ((n_x - 1)*s_x^2 + (n_y - 1)*s_y^2) / (n_x + n_y - 2)
sp <- sqrt(sp_2)
ts <- qt(1 - (alpha/2), n_x + n_y - 2)
round((x_bar - y_bar) + c(-1, 1) * ts * sp * (sqrt(1/n_x + 1/n_y)), 2)
n_x <- 9
n_y <- 9
x_bar <- -3
y_bar <- 1
s_x <- 1.5
s_y <- 1.8
alpha <- 0.1
sp_2 <- ((n_x - 1)*s_x^2 + (n_y - 1)*s_y^2) / (n_x + n_y - 2)
sp <- sqrt(sp_2)
ts <- qt(1 - (alpha/2), n_x + n_y - 2)
round((x_bar - y_bar) + c(-1, 1) * ts * sp * (sqrt(1/n_x + 1/n_y)), 3)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit) # as.factor(cyl)8  -6.0709
fit2 <- lm(mpg ~ as.factor(cyl), data=mtcars)
summary(fit2)$coef[3] # -11.56364
summary(fit)$coef[3] # -6.07086
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr # 0.1037502
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data=mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5] # 0.9945734
dfbetas(fit5)[5, 2] # -133.8226
library(AppliedPredictiveModeling)
data(segmentationOriginal)
rm(list = ls())
library(AppliedPredictiveModeling)
data(segmentationOriginal)
View(segmentationOriginal)
library(caret)
data <- segmentationOriginal
set.seed(125)
inTrain <- data$Case == "Train"
trainData <- data[inTrain, ]
testData <- data[!inTrain, ]
cartModel <- train(Class ~ ., data=trainData, method="rpart")
cartModel$finalModel
plot(cartModel$finalModel, uniform=T)
text(cartModel$finalModel, cex=0.8)
library(pgmm)
install.packages("pgmm")
library(pgmm)
data(olive)
dim(olive)
head(olive)
olive <- olive[,-1]
treeModel <- train(Area ~ ., data=olive, method="rpart2")
treeModel
newdata <- as.data.frame(t(colMeans(olive)))
predict(treeModel, newdata) # 2.875
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train <- sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA <- SAheart[train,]
testSA <- SAheart[-train,]
set.seed(13234)
logitModel <- train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl, data=trainSA, method="glm",
family="binomial")
logitModel
missClass <- function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predictTrain <- predict(logitModel, trainSA)
predictTest <- predict(logitModel, testSA)
# Training Set Misclassification rate
missClass(trainSA$chd, predictTrain) # 0.2727273
# Test Set Misclassification rate
missClass(testSA$chd, predictTest) # 0.3116883
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
install.packages("randomForest")
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
set.seed(3)
lambda <- 0.2
num_sim <- 1000
sample_size <- 40
sim <- matrix(rexp(num_sim*sample_size, rate=lambda), num_sim, sample_size)
row_means <- rowMeans(sim)
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
lines(density(row_means))
abline(v=1/lambda, col="red")
xfit <- seq(min(row_means), max(row_means), length=100)
yfit <- dnorm(xfit, mean=1/lambda, sd=(1/lambda/sqrt(sample_size)))
lines(xfit, yfit, pch=22, col="red", lty=2)
legend('topright', c("simulation", "theoretical"), lty=c(1,2), col=c("black", "red"))
qqnorm(row_means); qqline(row_means)
qqnorm(row_means); qqline(row_means)
lambda_vals <- seq(4, 6, by=0.01)
coverage <- sapply(lambda_vals, function(lamb) {
mu_hats <- rowMeans(matrix(rexp(sample_size*num_sim, rate=0.2),
num_sim, sample_size))
ll <- mu_hats - qnorm(0.975) * sqrt(1/lambda**2/sample_size)
ul <- mu_hats + qnorm(0.975) * sqrt(1/lambda**2/sample_size)
mean(ll < lamb & ul > lamb)
})
library(ggplot2)
qplot(lambda_vals, coverage) + geom_hline(yintercept=0.95)
library(datasets)
library(ggplot2)
ggplot(data=ToothGrowth, aes(x=as.factor(dose), y=len, fill=supp)) +
geom_bar(stat="identity",) +
facet_grid(. ~ supp) +
xlab("Dose in miligrams") +
ylab("Tooth length") +
guides(fill=guide_legend(title="Supplement type"))
fit <- lm(len ~ dose + supp, data=ToothGrowth)
summary(fit)
confint(fit)
install.packages("shiny")
library('shiny')
library(shiny)
library(shiny)
shinyUI(
navbarPage("Storm Database Explorer",
tabPanel("Plot",
sidebarPanel(
sliderInput("range",
"Range:",
min = 1950,
max = 2011,
value = c(1993, 2011),
format="####"),
uiOutput("evtypeControls")
),
mainPanel(
tabsetPanel(
# Data by state
tabPanel('By state',
column(3,
wellPanel(
radioButtons(
"populationCategory",
"Population impact category:",
c("Both" = "both", "Injuries" = "injuries", "Fatalities" = "fatalities"))
)
),
column(3,
wellPanel(
radioButtons(
"economicCategory",
"Economic impact category:",
c("Both" = "both", "Property damage" = "property", "Crops damage" = "crops"))
)
),
column(7,
plotOutput("populationImpactByState"),
plotOutput("economicImpactByState")
)
),
# Time series data
tabPanel('By year',
h4('Number of events by year', align = "center"),
showOutput("eventsByYear", "nvd3"),
h4('Population impact by year', align = "center"),
showOutput("populationImpact", "nvd3"),
h4('Economic impact by year', align = "center"),
showOutput("economicImpact", "nvd3")
),
# Data
tabPanel('Data',
dataTableOutput(outputId="table"),
downloadButton('downloadData', 'Download')
)
)
)
),
tabPanel("About",
mainPanel(
includeMarkdown("include.md")
)
)
)
)
)
shinyUI(
navbarPage("Storm Database Explorer",
tabPanel("Plot",
sidebarPanel(
sliderInput("range",
"Range:",
min = 1950,
max = 2011,
value = c(1993, 2011),
format="####"),
uiOutput("evtypeControls")
),
mainPanel(
tabsetPanel(
# Data by state
tabPanel('By state',
column(3,
wellPanel(
radioButtons(
"populationCategory",
"Population impact category:",
c("Both" = "both", "Injuries" = "injuries", "Fatalities" = "fatalities"))
)
),
column(3,
wellPanel(
radioButtons(
"economicCategory",
"Economic impact category:",
c("Both" = "both", "Property damage" = "property", "Crops damage" = "crops"))
)
),
column(7,
plotOutput("populationImpactByState"),
plotOutput("economicImpactByState")
)
),
# Time series data
tabPanel('By year',
h4('Number of events by year', align = "center"),
showOutput("eventsByYear", "nvd3"),
h4('Population impact by year', align = "center"),
showOutput("populationImpact", "nvd3"),
h4('Economic impact by year', align = "center"),
showOutput("economicImpact", "nvd3")
),
# Data
tabPanel('Data',
dataTableOutput(outputId="table"),
downloadButton('downloadData', 'Download')
)
)
)
),
tabPanel("About",
mainPanel(
includeMarkdown("include.md")
)
)
)
)
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54,
3.87, 4.97)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(devtools)
library(devtools)
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
setwd('C:/Users/vouri_000/Desktop/coursera/9-Developing Data Products')
author("first deck")
author("first deck")
author <- function(deckdir, use_git = TRUE, open_rmd = TRUE,
scaffold = system.file('skeleton', package = 'slidify')){
message('Creating slide directory at ', deckdir, '...')
#   if (file.exists(deckdir)){
#     return('Directory already exists. Please choose a different name.')
#   }
copy_dir(scaffold, deckdir)
message('Finished creating slide directory...')
message('Switching to slide directory...')
setwd(deckdir)
if (use_git && Sys.which('git') != ""){
init_repo()
}
if (open_rmd){
message('Opening slide deck...')
file.edit('index.Rmd')
}
}
#' Initialize a git repository, create and switch to gh-pages branch.
#'
#' @keywords internal
#' @noRd
init_repo <- function(){
message('Initializing Git Repo')
system("git init")
system("git commit --allow-empty -m 'Initial Commit'")
message("Checking out gh-pages branch...")
system('git checkout -b gh-pages')
message('Adding .nojekyll to repo')
file.create('.nojekyll')
}
author("first deck")
author("first deck")
install.packages("devtools")
library(devtools)
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
library(slidify)
setwd('C:/Users/vouri_000/Desktop/coursera/9-Developing Data Products')
author("first deck")
library(slidify)
author('Vouris Angelos')
slidify('index.Rmd')
library(knitr)
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
setwd('C:/Users/vouri_000/Desktop/coursera/9-Developing Data Products')
browseURL('index.html')
browseURL('index.html')
slidify('index.Rmd')
setwd('C:/Users/vouri_000/Desktop/coursera/9-Developing Data Products')
slidify('index.Rmd')
setwd('C:/Users/vouri_000/Desktop/coursera/9-Developing Data Products/first deck/Vouris Angelos')
slidify('index.Rmd')
browseURL('index.html')
install_version("stringr", version="0.6.2", type = "source")
browseURL('index.html')
slidify('index.Rmd')
